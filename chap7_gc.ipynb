{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chap7_gc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNw7H5L6S5+NR+ILNMeOSAc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaggiePN92/fastai/blob/master/chap7_gc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C3kCPWw6X8J",
        "outputId": "60f1da63-f4a3-43a8-ea86-cbc0ec4950aa"
      },
      "source": [
        "!pip install fastai --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastai\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/53/edf39e15b7ec5e805a0b6f72adbe48497ebcfa009a245eca7044ae9ee1c6/fastai-2.3.0-py3-none-any.whl (193kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 17.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 17.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 51kB 11.3MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 61kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 71kB 11.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 81kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 92kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 102kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 112kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 122kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 133kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 143kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 153kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 163kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 174kB 10.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 184kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from fastai) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from fastai) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai) (3.13)\n",
            "Collecting torchvision<0.9,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/df/969e69a94cff1c8911acb0688117f95e1915becc1e01c73e7960a2c76ec8/torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 208kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from fastai) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: spacy<3 in /usr/local/lib/python3.7/dist-packages (from fastai) (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.0)\n",
            "Collecting fastcore<1.4,>=1.3.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/98/60404e2817cff113a6ae4023bc1772e23179408fdf7857fa410551758dfe/fastcore-1.3.19-py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n",
            "\u001b[?25hCollecting torch<1.8,>=1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 23kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from fastai) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: pip in /usr/local/lib/python3.7/dist-packages (from fastai) (19.3.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (54.2.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3->fastai) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.8,>=1.7.0->fastai) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->fastai) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3->fastai) (3.8.1)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3->fastai) (3.4.1)\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision, fastcore, fastai\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "  Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "Successfully installed fastai-2.3.0 fastcore-1.3.19 torch-1.7.1 torchvision-0.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKH793Q1lYBL"
      },
      "source": [
        "#Training a State-of-the-Art Model\n",
        "\n",
        "Introduces:\n",
        "- normalization\n",
        "- Mixup\n",
        "- progressive resizing\n",
        "- test time augmentation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BARkoqfh6cNN"
      },
      "source": [
        "from fastai.vision.all import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "MnHHSahwnRJ_",
        "outputId": "9cd0fb19-7815-4541-ccf0-4f708db37f7d"
      },
      "source": [
        "path = untar_data(URLs.IMAGENETTE)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs_jJq9xndN8"
      },
      "source": [
        "dblock = DataBlock(\n",
        "    blocks=(ImageBlock(), CategoryBlock()),\n",
        "    get_items=get_image_files,\n",
        "    get_y=parent_label,\n",
        "    item_tfms=Resize(460),\n",
        "    batch_tfms=aug_transforms(size=224, min_scale=0.75)\n",
        ")\n",
        "dls = dblock.dataloaders(path, bs=64)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "XMaszbrWoBB9",
        "outputId": "a975376d-295a-4f72-b9cd-f02b1c43a2c6"
      },
      "source": [
        "#using un-pretrained model as baseline\n",
        "model = xresnet50()\n",
        "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
        "learn.fit_one_cycle(5, 3e-3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.582231</td>\n",
              "      <td>2.929521</td>\n",
              "      <td>0.362584</td>\n",
              "      <td>05:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.211608</td>\n",
              "      <td>3.735433</td>\n",
              "      <td>0.323749</td>\n",
              "      <td>05:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.952245</td>\n",
              "      <td>1.588585</td>\n",
              "      <td>0.576550</td>\n",
              "      <td>05:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.722617</td>\n",
              "      <td>0.659636</td>\n",
              "      <td>0.796863</td>\n",
              "      <td>05:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.589602</td>\n",
              "      <td>0.552289</td>\n",
              "      <td>0.834205</td>\n",
              "      <td>05:10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-jZ0z-IobBD",
        "outputId": "f645b348-c24c-4dcf-af9c-1bc0d768e05e"
      },
      "source": [
        "#normalize data\n",
        "x,y = dls.one_batch()\n",
        "x.mean(dim=[0,2,3]), x.std(dim=[0,2,3])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorImage([0.4834, 0.4915, 0.4598], device='cuda:0'),\n",
              " TensorImage([0.2723, 0.2677, 0.3010], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qOCi2i8rz9A",
        "outputId": "85d47935-fdd9-4cb7-d449-c59e2e5ed528"
      },
      "source": [
        "def get_dls(bs, size):\n",
        "  dblock = DataBlock(\n",
        "      blocks=(ImageBlock(), CategoryBlock()),\n",
        "      get_items=get_image_files,\n",
        "      get_y=parent_label,\n",
        "      item_tfms=Resize(460),\n",
        "      batch_tfms=[*aug_transforms(size=size, min_scale=0.75),\n",
        "                  Normalize.from_stats(*imagenet_stats)]\n",
        "  )\n",
        "  return dblock.dataloaders(path, bs=bs)\n",
        "\n",
        "dls = get_dls(64, 224)\n",
        "\n",
        "x,y = dls.one_batch()\n",
        "x.mean(dim=[0,2,3]), x.std(dim=[0,2,3])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorImage([-0.1730, -0.0487,  0.0951], device='cuda:0'),\n",
              " TensorImage([1.2039, 1.2084, 1.3074], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "bNC6hXcar6Xz",
        "outputId": "005c6eac-909c-430e-c253-ec3feda1b16d"
      },
      "source": [
        "#using un-pretrained model as baseline, now w/ norm data\n",
        "model = xresnet50()\n",
        "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
        "learn.fit_one_cycle(5, 3e-3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.653275</td>\n",
              "      <td>1.752058</td>\n",
              "      <td>0.479089</td>\n",
              "      <td>05:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.251967</td>\n",
              "      <td>1.276721</td>\n",
              "      <td>0.632188</td>\n",
              "      <td>05:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.937295</td>\n",
              "      <td>1.124018</td>\n",
              "      <td>0.654593</td>\n",
              "      <td>05:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.731189</td>\n",
              "      <td>0.691861</td>\n",
              "      <td>0.789395</td>\n",
              "      <td>05:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.576136</td>\n",
              "      <td>0.538391</td>\n",
              "      <td>0.833831</td>\n",
              "      <td>05:10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVW5VasJrhO8"
      },
      "source": [
        "#Progressive resizing: \n",
        "gradually using larger and larger images as you train. <br>\n",
        "Start small bc training is faster, end with larger as acc gets better. <br> <br>\n",
        "\n",
        "\n",
        "Can hurt performance if data and task is similar to pretrained model. On the contrary, if task and data is different; will probably improve performance. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "5GNzZ9XArS9J",
        "outputId": "995262d1-7ef9-43c7-882e-19a97c55af52"
      },
      "source": [
        "#progressive resizing:\n",
        "dls = get_dls(128,128)\n",
        "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
        "learn.fit_one_cycle(4, 3e-3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.772921</td>\n",
              "      <td>0.933580</td>\n",
              "      <td>0.707618</td>\n",
              "      <td>03:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.771989</td>\n",
              "      <td>1.021460</td>\n",
              "      <td>0.678118</td>\n",
              "      <td>02:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.626747</td>\n",
              "      <td>0.567813</td>\n",
              "      <td>0.820015</td>\n",
              "      <td>02:57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.494075</td>\n",
              "      <td>0.468738</td>\n",
              "      <td>0.852502</td>\n",
              "      <td>02:59</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "G5JY6p0NuX2w",
        "outputId": "4a2fe3bc-c64c-4759-842a-ab38429e3203"
      },
      "source": [
        "learn.dls = get_dls(64, 224)\n",
        "learn.fine_tune(5,1e-3)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.545200</td>\n",
              "      <td>0.889536</td>\n",
              "      <td>0.733010</td>\n",
              "      <td>05:12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.449443</td>\n",
              "      <td>0.437027</td>\n",
              "      <td>0.862957</td>\n",
              "      <td>05:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.443790</td>\n",
              "      <td>0.405572</td>\n",
              "      <td>0.867065</td>\n",
              "      <td>05:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.413028</td>\n",
              "      <td>0.400557</td>\n",
              "      <td>0.870052</td>\n",
              "      <td>05:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.345883</td>\n",
              "      <td>0.328906</td>\n",
              "      <td>0.892830</td>\n",
              "      <td>05:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.304500</td>\n",
              "      <td>0.321251</td>\n",
              "      <td>0.896191</td>\n",
              "      <td>05:10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahE-cqrksUp8"
      },
      "source": [
        "#Test time augmentation\n",
        "Untill now we have been using random cropping as data augmentation, this leads to better generalization. This can be problematic, especially in multilabel datasets. Important parts of the image might be dropped out. A possible solution is to squash or stretch imgs. This makes it harder to learn shapes and we miss out on important img augmentation. \n",
        "\n",
        "TTA: \n",
        "Select number of areas to crop, pass through model, take max/avg of preds. Do this for diff values across TTA params. Will not increase training time, but will incrase valididation time.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckhnIcQQugPN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "f31bc165-43a2-473d-df58-31159750adf1"
      },
      "source": [
        "preds, targs = learn.tta()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      \n",
              "    </div>\n",
              "    \n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viHk4-wVquUH",
        "outputId": "f9fef64e-7e6d-4bf9-bc78-5899ab85a08c"
      },
      "source": [
        "accuracy(preds, targs).item()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9006721377372742"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN_y6UKu1U5u"
      },
      "source": [
        "#Mixup\n",
        "Can improve performance when:\n",
        "- don't have much data\n",
        "- have pretrained model that was trained on different data\n",
        "\n",
        "Data augmentation is important for generalization. How much depends on your data. Dialing how much augmentation to do can lead to better results. \n",
        "\n",
        "Mixup works in the following way, for each image:\n",
        "1. Select another img from dataset at random\n",
        "2. Pick a random weight\n",
        "3. Take weighted avg of the two images, this is independent variable\n",
        "4. Take weighted avg of targets from the two imgs, this will be dependent variable\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaH83IM1zj2Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "8066d357-f586-4e19-c758-ca58747f5055"
      },
      "source": [
        "model = xresnet50()\n",
        "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(),\n",
        "                metrics=accuracy, cbs=MixUp) #cbs for callback\n",
        "learn.fit_one_cycle(5, 3e-3)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.195424</td>\n",
              "      <td>2.042561</td>\n",
              "      <td>0.396565</td>\n",
              "      <td>02:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.733193</td>\n",
              "      <td>1.385232</td>\n",
              "      <td>0.571322</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.490590</td>\n",
              "      <td>1.028455</td>\n",
              "      <td>0.660568</td>\n",
              "      <td>02:45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.326894</td>\n",
              "      <td>0.791496</td>\n",
              "      <td>0.760269</td>\n",
              "      <td>02:44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.203354</td>\n",
              "      <td>0.699952</td>\n",
              "      <td>0.796117</td>\n",
              "      <td>02:45</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8MrgqWNv7Ul"
      },
      "source": [
        "When we use mixup the model:\n",
        "- will be harder to train, thus needs _many_ more epochs.\n",
        "- will be less likely to overfit, because imgs will be different each epoch\n",
        "\n",
        "Mixup is not exclusively for image recognition. It has also been used in NLP and other types of data. \n",
        "\n",
        "Mixup also addresses the issue that loss never will be perfect. The labels in the data is either 0 or 1 (given binary clf.), but the output of sigmoid will be in the range (0,1). When we train our model, the model will make more extreme predictions for each epoch. With mixup the label will also be in the range (0,1), given that it doesn't mix two of the same class. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROFz9ebQ10r8"
      },
      "source": [
        "#Label smoothing\n",
        "Labels for training data in a binary clf. is usually one-hot encoded, meaning they're either 0 or 1. Because of this the model is trained to predict either 0 or 1. Even 0.999 isn't good enough, and the optimization algo will try to improve this prediction. This can be harmfull and lead to overfitting, especially when you have mislabeled data. \n",
        "\n",
        "Instead of having labels that is either 0 or 1, we could have labels which are slightly higher than 0 and slightly lower than 1. This will make inference more robust, even if there is mislabeled data. \n",
        "\n",
        "When using label smoothing 0's will be replaced with $\\epsilon / N$, where $N = $ # classes. The 1's will be replaced by $1 - \\epsilon + (\\epsilon / N)$\n",
        "\n",
        "When you use label smoothing your model will be less likely to have large weights and activations. Large weights can lead to bumpy functions, where a small change in input might lead to big change in output. \n",
        "\n",
        "When the labels are 0 or 1 and gradients are bounded to -1 and 1, transfer learning becomes harder. This is because loss due to incorrect preds are unbounded, but we can only take a limited step each time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "I8Sqws01ven6",
        "outputId": "da7176ee-c3ab-4246-f40d-a9cb91eac411"
      },
      "source": [
        "model = xresnet50()\n",
        "learn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropy(),\n",
        "                metrics=accuracy)\n",
        "learn.fit_one_cycle(5, 3e-3)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.767864</td>\n",
              "      <td>3.278496</td>\n",
              "      <td>0.416729</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.254204</td>\n",
              "      <td>2.394877</td>\n",
              "      <td>0.515683</td>\n",
              "      <td>02:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.966359</td>\n",
              "      <td>1.917373</td>\n",
              "      <td>0.681105</td>\n",
              "      <td>02:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.776351</td>\n",
              "      <td>1.812582</td>\n",
              "      <td>0.744586</td>\n",
              "      <td>02:45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.636841</td>\n",
              "      <td>1.582906</td>\n",
              "      <td>0.810306</td>\n",
              "      <td>02:45</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRxND4eJJlGE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}